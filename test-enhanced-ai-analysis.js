// Test script for Enhanced AI Risk Analysis System
// This script validates the enhanced AI analysis with evidence-based scoring

import { 
  generateAnalysisWithCitations, 
  generateStructuredPrompts,
  simulateAIAnalysisWithPrompts 
} from './src/lib/enhanced-risk-ai-analysis.js'
import { EvidenceCitationTracker } from './src/lib/evidence-citation-tracker.js'
import { StructuredPromptBuilder } from './src/lib/structured-prompt-builder.js'

// Mock evaluation data for testing
const mockEvaluations = [
  {
    id: 'eval-001',
    title: 'Ã‰valuation SÃ©curitÃ© Site Minier Alpha',
    status: 'COMPLETED',
    totalScore: 65,
    riskLevel: 'MEDIUM',
    entityInfo: {
      sector: 'mining',
      companySize: 'large',
      location: 'CÃ´te d\'Ivoire'
    },
    template: {
      name: 'Ã‰valuation SÃ©curitÃ© ComplÃ¨te'
    },
    responses: [
      {
        id: 'resp-001',
        questionId: 'q-maintenance',
        questionText: 'Existe-t-il un service de maintenance prÃ©ventive?',
        booleanValue: false,
        description: 'Aucun service de maintenance organisÃ©',
        answeredAt: '2024-01-15T10:00:00Z'
      },
      {
        id: 'resp-002',
        questionId: 'q-formation',
        questionText: 'Quel pourcentage du personnel est formÃ© aux procÃ©dures de sÃ©curitÃ©?',
        textValue: '93% du personnel formÃ©',
        description: 'Formation annuelle obligatoire',
        answeredAt: '2024-01-15T10:05:00Z'
      },
      {
        id: 'resp-003',
        questionId: 'q-surveillance',
        questionText: 'Y a-t-il une surveillance nocturne du pÃ©rimÃ¨tre?',
        booleanValue: false,
        description: 'Pas de surveillance entre 22h et 6h',
        answeredAt: '2024-01-15T10:10:00Z'
      },
      {
        id: 'resp-004',
        questionId: 'q-cloture',
        questionText: 'Le pÃ©rimÃ¨tre est-il entiÃ¨rement clÃ´turÃ©?',
        booleanValue: true,
        description: 'ClÃ´ture sur 80% du pÃ©rimÃ¨tre',
        answeredAt: '2024-01-15T10:15:00Z'
      },
      {
        id: 'resp-005',
        questionId: 'q-communication',
        questionText: 'Existe-t-il un systÃ¨me de communication d\'urgence?',
        booleanValue: false,
        description: 'Pas de systÃ¨me dÃ©diÃ©',
        answeredAt: '2024-01-15T10:20:00Z'
      }
    ],
    completedAt: '2024-01-15T11:00:00Z'
  },
  {
    id: 'eval-002',
    title: 'Audit Infrastructure Site Alpha',
    status: 'COMPLETED',
    totalScore: 45,
    riskLevel: 'HIGH',
    entityInfo: {
      sector: 'mining',
      companySize: 'large'
    },
    template: {
      name: 'Audit Infrastructure'
    },
    responses: [
      {
        id: 'resp-006',
        questionId: 'q-infrastructure',
        questionText: 'Ã‰tat gÃ©nÃ©ral de l\'infrastructure Ã©lectrique',
        facilityScore: 2,
        constraintScore: 4,
        description: 'Infrastructure vieillissante',
        answeredAt: '2024-01-20T14:00:00Z'
      },
      {
        id: 'resp-007',
        questionId: 'q-redondance',
        questionText: 'SystÃ¨mes de redondance en place?',
        booleanValue: true,
        description: 'GÃ©nÃ©rateurs de secours disponibles',
        answeredAt: '2024-01-20T14:05:00Z'
      }
    ],
    completedAt: '2024-01-20T15:00:00Z'
  }
]

const mockRiskData = {
  target: 'SystÃ¨me de surveillance pÃ©rimÃ©trique nocturne',
  scenario: 'DÃ©faillance du systÃ¨me de surveillance permettant une intrusion non dÃ©tectÃ©e',
  category: 'SÃ©curitÃ© physique'
}

// Test functions
async function testEnhancedDataExtraction() {
  console.log('\n=== Test 1: Enhanced Data Extraction ===')
  
  try {
    const { EnhancedDataExtractionEngine } = await import('./src/lib/enhanced-risk-ai-analysis.js')
    const dataExtractor = new EnhancedDataExtractionEngine()
    
    const insights = dataExtractor.extractEvaluationInsights(mockEvaluations)
    
    console.log('âœ… Data extraction successful')
    console.log(`ğŸ“Š Total responses: ${insights.totalResponses}`)
    console.log(`ğŸ“‹ Total evaluations: ${insights.totalEvaluations}`)
    console.log(`ğŸ¯ Average score: ${insights.averageScore.toFixed(1)}`)
    console.log(`ğŸ¢ Sector context: ${insights.sectorContext}`)
    console.log(`ğŸ“ˆ Company maturity: ${insights.companyMaturity}`)
    console.log(`ğŸ” Evidence quality: ${insights.evidenceQuality.toFixed(1)}%`)
    console.log(`ğŸ“ Extracted metrics: ${insights.extractedMetrics.length}`)
    console.log(`ğŸ”— Patterns identified: ${insights.patterns.length}`)
    console.log(`âš ï¸  Critical weaknesses: ${insights.criticalWeaknesses.length}`)
    console.log(`âœ¨ Strength areas: ${insights.strengthAreas.length}`)
    
    // Validate domain scores
    console.log('\nğŸ“Š Domain Scores:')
    Object.entries(insights.domainScores).forEach(([domain, score]) => {
      console.log(`  - ${domain}: ${score.toFixed(1)}%`)
    })
    
    return insights
  } catch (error) {
    console.error('âŒ Data extraction failed:', error.message)
    return null
  }
}

async function testEvidenceCitationTracker() {
  console.log('\n=== Test 2: Evidence Citation Tracker ===')
  
  try {
    const evidenceTracker = new EvidenceCitationTracker()
    evidenceTracker.addEvidenceFromEvaluations(mockEvaluations)
    
    // Test evidence finding
    const probabilityEvidence = evidenceTracker.findRelevantEvidence('probability', 5)
    const vulnerabilityEvidence = evidenceTracker.findRelevantEvidence('vulnerability', 5)
    const impactEvidence = evidenceTracker.findRelevantEvidence('impact', 5)
    
    console.log('âœ… Evidence tracking successful')
    console.log(`ğŸ¯ Probability evidence: ${probabilityEvidence.length} items`)
    console.log(`ğŸ›¡ï¸  Vulnerability evidence: ${vulnerabilityEvidence.length} items`)
    console.log(`ğŸ’¥ Impact evidence: ${impactEvidence.length} items`)
    
    // Test citation creation
    probabilityEvidence.forEach(evidence => {
      evidenceTracker.createCitation(evidence.id, 'probability', 'negative')
    })
    
    const citations = evidenceTracker.getFormattedCitations('probability')
    console.log(`ğŸ“ Generated citations: ${citations.negative.length}`)
    
    // Test validation
    const validation = evidenceTracker.validateCitations()
    console.log(`âœ… Citation validation: ${validation.isValid ? 'PASSED' : 'FAILED'}`)
    if (validation.issues.length > 0) {
      console.log(`âš ï¸  Issues found: ${validation.issues.length}`)
    }
    
    // Test summary
    const summary = evidenceTracker.getEvidenceSummary()
    console.log(`ğŸ“Š Evidence summary: ${summary.totalEvidence} total items`)
    console.log(`ğŸ¯ Average confidence: ${(summary.averageConfidence * 100).toFixed(1)}%`)
    console.log(`ğŸ“ˆ Average relevance: ${(summary.averageRelevance * 100).toFixed(1)}%`)
    
    return evidenceTracker
  } catch (error) {
    console.error('âŒ Evidence citation tracking failed:', error.message)
    return null
  }
}

async function testStructuredPromptBuilder() {
  console.log('\n=== Test 3: Structured Prompt Builder ===')
  
  try {
    const promptBuilder = new StructuredPromptBuilder()
    const insights = await testEnhancedDataExtraction()
    
    if (!insights) {
      throw new Error('No insights available for prompt building')
    }
    
    const promptContext = {
      target: mockRiskData.target,
      scenario: mockRiskData.scenario,
      category: mockRiskData.category,
      evaluationInsights: insights,
      analysisType: 'probability'
    }
    
    // Test all prompt types
    const probabilityPrompt = promptBuilder.buildProbabilityPrompt(promptContext)
    const vulnerabilityPrompt = promptBuilder.buildVulnerabilityPrompt({
      ...promptContext,
      analysisType: 'vulnerability'
    })
    const impactPrompt = promptBuilder.buildImpactPrompt({
      ...promptContext,
      analysisType: 'impact'
    })
    
    console.log('âœ… Prompt generation successful')
    console.log(`ğŸ“ Probability prompt length: ${probabilityPrompt.userPrompt.length} chars`)
    console.log(`ğŸ›¡ï¸  Vulnerability prompt length: ${vulnerabilityPrompt.userPrompt.length} chars`)
    console.log(`ğŸ’¥ Impact prompt length: ${impactPrompt.userPrompt.length} chars`)
    
    // Validate prompt structure
    const hasSystemPrompt = probabilityPrompt.systemPrompt.length > 0
    const hasUserPrompt = probabilityPrompt.userPrompt.length > 0
    const hasOutputFormat = probabilityPrompt.outputFormat.length > 0
    const hasExamples = probabilityPrompt.examples.length > 0
    
    console.log(`ğŸ”§ System prompt: ${hasSystemPrompt ? 'âœ…' : 'âŒ'}`)
    console.log(`ğŸ‘¤ User prompt: ${hasUserPrompt ? 'âœ…' : 'âŒ'}`)
    console.log(`ğŸ“‹ Output format: ${hasOutputFormat ? 'âœ…' : 'âŒ'}`)
    console.log(`ğŸ“š Examples: ${hasExamples ? 'âœ…' : 'âŒ'}`)
    
    return { probabilityPrompt, vulnerabilityPrompt, impactPrompt }
  } catch (error) {
    console.error('âŒ Structured prompt building failed:', error.message)
    return null
  }
}

async function testCompleteAnalysisWorkflow() {
  console.log('\n=== Test 4: Complete Analysis Workflow ===')
  
  try {
    console.log('ğŸš€ Starting complete analysis workflow...')
    
    // Test the complete analysis with citations
    const analysis = await generateAnalysisWithCitations(mockRiskData, mockEvaluations)
    
    console.log('âœ… Complete analysis successful')
    console.log(`ğŸ¯ Probability score: ${analysis.probability.score}/3 (${(analysis.probability.confidence * 100).toFixed(1)}% confidence)`)
    console.log(`ğŸ›¡ï¸  Vulnerability score: ${analysis.vulnerability.score}/4 (${(analysis.vulnerability.confidence * 100).toFixed(1)}% confidence)`)
    console.log(`ğŸ’¥ Impact score: ${analysis.impact.score}/5 (${(analysis.impact.confidence * 100).toFixed(1)}% confidence)`)
    console.log(`ğŸ† Reasoning quality: ${analysis.reasoningQuality}`)
    
    // Validate evidence points
    const hasPositivePoints = analysis.probability.positivePoints.length > 0 || 
                             analysis.vulnerability.positivePoints.length > 0 || 
                             analysis.impact.positivePoints.length > 0
    
    const hasNegativePoints = analysis.probability.negativePoints.length > 0 || 
                             analysis.vulnerability.negativePoints.length > 0 || 
                             analysis.impact.negativePoints.length > 0
    
    console.log(`âœ¨ Positive evidence: ${hasPositivePoints ? 'âœ…' : 'âŒ'}`)
    console.log(`âš ï¸  Negative evidence: ${hasNegativePoints ? 'âœ…' : 'âŒ'}`)
    
    // Check for citations in evidence points
    const hasCitations = analysis.probability.positivePoints.some(point => point.includes('Source:')) ||
                        analysis.probability.negativePoints.some(point => point.includes('Source:'))
    
    console.log(`ğŸ“ Evidence citations: ${hasCitations ? 'âœ…' : 'âŒ'}`)
    
    // Validate overall assessment
    const hasOverallAssessment = analysis.overallAssessment && analysis.overallAssessment.length > 100
    console.log(`ğŸ“Š Overall assessment: ${hasOverallAssessment ? 'âœ…' : 'âŒ'}`)
    
    // Validate questionnaire recommendations
    const hasRecommendations = analysis.questionnaireRecommendations && analysis.questionnaireRecommendations.length > 0
    console.log(`ğŸ’¡ Questionnaire recommendations: ${hasRecommendations ? 'âœ…' : 'âŒ'}`)
    
    // Display sample evidence
    console.log('\nğŸ“‹ Sample Evidence Points:')
    if (analysis.probability.negativePoints.length > 0) {
      console.log(`  Probability (negative): ${analysis.probability.negativePoints[0]}`)
    }
    if (analysis.vulnerability.positivePoints.length > 0) {
      console.log(`  Vulnerability (positive): ${analysis.vulnerability.positivePoints[0]}`)
    }
    
    return analysis
  } catch (error) {
    console.error('âŒ Complete analysis workflow failed:', error.message)
    console.error(error.stack)
    return null
  }
}

// Performance test
async function testPerformance() {
  console.log('\n=== Test 5: Performance Test ===')
  
  try {
    const startTime = Date.now()
    
    await generateAnalysisWithCitations(mockRiskData, mockEvaluations)
    
    const endTime = Date.now()
    const duration = endTime - startTime
    
    console.log(`â±ï¸  Analysis completed in ${duration}ms`)
    console.log(`ğŸš€ Performance: ${duration < 5000 ? 'âœ… GOOD' : duration < 10000 ? 'âš ï¸  ACCEPTABLE' : 'âŒ SLOW'}`)
    
    return duration
  } catch (error) {
    console.error('âŒ Performance test failed:', error.message)
    return null
  }
}

// Main test runner
async function runAllTests() {
  console.log('ğŸ§ª Enhanced AI Risk Analysis System - Test Suite')
  console.log('================================================')
  
  const results = {
    dataExtraction: false,
    evidenceTracking: false,
    promptBuilding: false,
    completeWorkflow: false,
    performance: false
  }
  
  try {
    // Run all tests
    const insights = await testEnhancedDataExtraction()
    results.dataExtraction = insights !== null
    
    const evidenceTracker = await testEvidenceCitationTracker()
    results.evidenceTracking = evidenceTracker !== null
    
    const prompts = await testStructuredPromptBuilder()
    results.promptBuilding = prompts !== null
    
    const analysis = await testCompleteAnalysisWorkflow()
    results.completeWorkflow = analysis !== null
    
    const performance = await testPerformance()
    results.performance = performance !== null && performance < 10000
    
    // Summary
    console.log('\nğŸ“Š Test Results Summary')
    console.log('========================')
    Object.entries(results).forEach(([test, passed]) => {
      console.log(`${passed ? 'âœ…' : 'âŒ'} ${test}: ${passed ? 'PASSED' : 'FAILED'}`)
    })
    
    const passedTests = Object.values(results).filter(Boolean).length
    const totalTests = Object.keys(results).length
    
    console.log(`\nğŸ† Overall: ${passedTests}/${totalTests} tests passed`)
    
    if (passedTests === totalTests) {
      console.log('ğŸ‰ All tests passed! Enhanced AI analysis system is ready.')
    } else {
      console.log('âš ï¸  Some tests failed. Please review the implementation.')
    }
    
  } catch (error) {
    console.error('âŒ Test suite failed:', error.message)
  }
}

// Run tests if this file is executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  runAllTests()
}

export { runAllTests, testCompleteAnalysisWorkflow, mockEvaluations, mockRiskData }
